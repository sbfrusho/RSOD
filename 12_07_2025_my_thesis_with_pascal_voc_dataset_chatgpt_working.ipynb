{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Libraries"
      ],
      "metadata": {
        "id": "aKAQNLlqK9ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaAvDTM9iDLR",
        "outputId": "f80e87f8-78a6-4b57-b2b7-e5a5ab804e34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # cell1 : install libraries.\n",
        "# # !pip install torch torchvision torchaudio\n",
        "# # !pip install albumentations opencv-python matplotlib numpy pandas pycocotools\n",
        "# # !pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
        "# # cell 2 : set dataset paths\n",
        "# import os\n",
        "# from glob import glob\n",
        "\n",
        "# train_img_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/train/images\"\n",
        "# train_ann_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/train/Annotations\"\n",
        "# val_img_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/val/images\"\n",
        "# val_ann_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/val/Annotations\"\n",
        "\n",
        "# assert os.path.exists(train_img_dir)\n",
        "# assert os.path.exists(train_ann_dir)\n",
        "# assert os.path.exists(val_img_dir)\n",
        "# assert os.path.exists(val_ann_dir)\n",
        "\n",
        "# print(\"Dataset paths verified successfully!\")\n",
        "# # cell 3: custom clahe transform\n",
        "# import cv2\n",
        "# import albumentations as A\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "# from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "\n",
        "# class CLAHETransform(ImageOnlyTransform):\n",
        "#     def __init__(self, clip_limit=2.0, tile_grid_size=(8,8), always_apply=False, p=1.0):\n",
        "#         super(CLAHETransform, self).__init__(always_apply=always_apply, p=p)\n",
        "#         self.clip_limit = clip_limit\n",
        "#         self.tile_grid_size = tile_grid_size\n",
        "\n",
        "#     def apply(self, image, **params):\n",
        "#         lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "#         l, a, b = cv2.split(lab)\n",
        "#         clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "#         cl = clahe.apply(l)\n",
        "#         merged = cv2.merge((cl, a, b))\n",
        "#         return cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "#     def get_transform_init_args_names(self):\n",
        "#         return (\"clip_limit\", \"tile_grid_size\")\n",
        "# # cell 4 : transforms\n",
        "# def get_train_transform():\n",
        "#     return A.Compose([\n",
        "#         CLAHETransform(),\n",
        "#         A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
        "#         A.HueSaturationValue(20, 30, 20, p=0.5),\n",
        "#         A.RGBShift(15, 15, 15, p=0.5),\n",
        "#         A.HorizontalFlip(p=0.5),\n",
        "#         A.VerticalFlip(p=0.5),\n",
        "#         A.RandomRotate90(p=0.5),\n",
        "#         A.Resize(640, 640),\n",
        "#         A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#         ToTensorV2()\n",
        "#     ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# def get_val_transform():\n",
        "#     return A.Compose([\n",
        "#         CLAHETransform(),\n",
        "#         A.Resize(640, 640),\n",
        "#         A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#         ToTensorV2()\n",
        "#     ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "# # cell 5: dataset class\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import xml.etree.ElementTree as ET\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# class RSODDataset(Dataset):\n",
        "#     def __init__(self, image_dir, annotation_dir, transforms=None):\n",
        "#         self.image_dir = image_dir\n",
        "#         self.annotation_dir = annotation_dir\n",
        "#         self.transforms = transforms\n",
        "#         self.image_files = sorted(glob(os.path.join(image_dir, \"*.jpg\")) + glob(os.path.join(image_dir, \"*.png\")))\n",
        "#         self.class_dict = {\"aircraft\": 0, \"oiltank\": 1, \"overpass\": 2, \"playground\": 3}\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_files)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.image_files[idx]\n",
        "#         image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "#         xml_path = os.path.join(self.annotation_dir, os.path.basename(img_path).replace(\".jpg\", \".xml\").replace(\".png\", \".xml\"))\n",
        "#         tree = ET.parse(xml_path)\n",
        "#         root = tree.getroot()\n",
        "\n",
        "#         boxes, labels = [], []\n",
        "#         for obj in root.findall('object'):\n",
        "#             class_name = obj.find('name').text.lower()\n",
        "#             if class_name not in self.class_dict:\n",
        "#                 continue\n",
        "#             bbox = obj.find('bndbox')\n",
        "#             xmin = float(bbox.find('xmin').text)\n",
        "#             ymin = float(bbox.find('ymin').text)\n",
        "#             xmax = float(bbox.find('xmax').text)\n",
        "#             ymax = float(bbox.find('ymax').text)\n",
        "#             boxes.append([xmin, ymin, xmax, ymax])\n",
        "#             labels.append(self.class_dict[class_name])\n",
        "\n",
        "#         boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4))\n",
        "#         labels = np.array(labels) if labels else np.zeros(0,)\n",
        "\n",
        "#         if self.transforms:\n",
        "#             transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
        "#             image = transformed[\"image\"]\n",
        "#             boxes = torch.as_tensor(transformed[\"bboxes\"], dtype=torch.float32)\n",
        "#             labels = torch.as_tensor(transformed[\"labels\"], dtype=torch.int64)\n",
        "\n",
        "#         return image, {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "# train_dataset = RSODDataset(train_img_dir, train_ann_dir, get_train_transform())\n",
        "# val_dataset = RSODDataset(val_img_dir, val_ann_dir, get_val_transform())\n",
        "\n",
        "# print(f\"Training samples: {len(train_dataset)}\")\n",
        "# print(f\"Validation samples: {len(val_dataset)}\")\n",
        "# # cell 6 : meodel setup\n",
        "# import torchvision\n",
        "# from torchvision.models.detection import FasterRCNN\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator\n",
        "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "# from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "# def get_model(num_classes):\n",
        "#     backbone = resnet_fpn_backbone('resnet101', pretrained=True, trainable_layers=5)\n",
        "#     anchor_generator = AnchorGenerator(sizes=((32,), (64,), (128,), (256,), (512,)),\n",
        "#                                        aspect_ratios=((0.5, 1.0, 2.0, 3.0),)*5)\n",
        "#     roi_pooler = MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=7, sampling_ratio=2)\n",
        "#     model = FasterRCNN(backbone, num_classes=num_classes,\n",
        "#                        rpn_anchor_generator=anchor_generator,\n",
        "#                        box_roi_pool=roi_pooler,\n",
        "#                        box_score_thresh=0.05)\n",
        "#     return model\n",
        "\n",
        "# model = get_model(num_classes=5)\n",
        "# # cell 7 : dataloader and optimizer\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     return tuple(zip(*batch))\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "# params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "# # cell 8 : training + evaluation with focal loss\n",
        "# import time\n",
        "# from torch.nn import functional as F\n",
        "\n",
        "# class FocalLoss(torch.nn.Module):\n",
        "#     def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
        "#         super(FocalLoss, self).__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.gamma = gamma\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "#         loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "#         return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
        "\n",
        "# def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     focal_loss = FocalLoss()\n",
        "\n",
        "#     for images, targets in data_loader:\n",
        "#         images = [img.to(device).float() for img in images]\n",
        "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "#         loss_dict = model(images, targets)\n",
        "\n",
        "#         losses = sum(loss for loss in loss_dict.values())\n",
        "#         optimizer.zero_grad()\n",
        "#         losses.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += losses.item()\n",
        "\n",
        "#     return total_loss / len(data_loader)\n",
        "\n",
        "# def evaluate(model, data_loader, device):\n",
        "#     model.eval()\n",
        "#     eval_loss = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for images, targets in data_loader:\n",
        "#             images = [img.to(device) for img in images]\n",
        "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "#             # Use model in training mode temporarily to compute loss\n",
        "#             model.train()\n",
        "#             loss_dict = model(images, targets)\n",
        "#             model.eval()\n",
        "\n",
        "#             losses = sum(loss for loss in loss_dict.values())\n",
        "#             eval_loss += losses.item()\n",
        "\n",
        "#     return eval_loss / len(data_loader)\n",
        "\n",
        "\n",
        "# num_epochs = 20\n",
        "# for epoch in range(num_epochs):\n",
        "#     start = time.time()\n",
        "#     train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "#     val_loss = evaluate(model, val_loader, device)\n",
        "#     scheduler.step()\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Time: {time.time()-start:.2f}s\")\n",
        "# # cell 9 : coco evaluation\n",
        "# from pycocotools.coco import COCO\n",
        "# from pycocotools.cocoeval import COCOeval\n",
        "# import torch\n",
        "# import json\n",
        "\n",
        "# def evaluate_coco(model, data_loader, device):\n",
        "#     model.eval()\n",
        "#     results = []\n",
        "#     image_ids = []\n",
        "\n",
        "#     with torch.no_grad():  # Important: disable gradients for inference\n",
        "#         for i, (images, targets) in enumerate(data_loader):\n",
        "#             images = [img.to(device) for img in images]\n",
        "#             outputs = model(images)\n",
        "\n",
        "#             for j, output in enumerate(outputs):\n",
        "#                 image_id = targets[j].get(\"image_id\", torch.tensor(i * len(images) + j)).item()\n",
        "#                 boxes = output[\"boxes\"].detach().cpu().numpy()   # ✅ detach() here\n",
        "#                 scores = output[\"scores\"].detach().cpu().numpy() # ✅ detach()\n",
        "#                 labels = output[\"labels\"].detach().cpu().numpy() # ✅ detach()\n",
        "\n",
        "#                 for k in range(len(boxes)):\n",
        "#                     result = {\n",
        "#                         \"image_id\": image_id,\n",
        "#                         \"category_id\": int(labels[k]),\n",
        "#                         \"bbox\": [\n",
        "#                             float(boxes[k][0]),\n",
        "#                             float(boxes[k][1]),\n",
        "#                             float(boxes[k][2] - boxes[k][0]),\n",
        "#                             float(boxes[k][3] - boxes[k][1])\n",
        "#                         ],\n",
        "#                         \"score\": float(scores[k])\n",
        "#                     }\n",
        "#                     results.append(result)\n",
        "\n",
        "#     with open(\"predictions.json\", \"w\") as f:\n",
        "#         json.dump(results, f)\n",
        "\n",
        "#     # Build fake COCO-style GT from val_dataset\n",
        "#     val_image_ids = list(range(len(val_dataset)))\n",
        "#     coco_gt = COCO()\n",
        "#     dataset = {\n",
        "#         \"info\": {\n",
        "#             \"description\": \"RSOD Validation\",\n",
        "#             \"version\": \"1.0\",\n",
        "#             \"year\": 2025\n",
        "#         },\n",
        "#         \"licenses\": [],\n",
        "#         \"images\": [{\"id\": i} for i in val_image_ids],\n",
        "#         \"categories\": [\n",
        "#             {\"id\": 0, \"name\": \"aircraft\"},\n",
        "#             {\"id\": 1, \"name\": \"oiltank\"},\n",
        "#             {\"id\": 2, \"name\": \"overpass\"},\n",
        "#             {\"id\": 3, \"name\": \"playground\"}\n",
        "#         ],\n",
        "#         \"annotations\": []\n",
        "#     }\n",
        "\n",
        "\n",
        "#     ann_id = 0\n",
        "#     for img_id in val_image_ids:\n",
        "#         _, target = val_dataset[img_id]\n",
        "#         boxes = target[\"boxes\"].detach().numpy()   # ✅ detach() here too\n",
        "#         labels = target[\"labels\"].detach().numpy()\n",
        "\n",
        "#         for k in range(len(boxes)):\n",
        "#             annotation = {\n",
        "#                 \"id\": ann_id,\n",
        "#                 \"image_id\": img_id,\n",
        "#                 \"category_id\": int(labels[k]),\n",
        "#                 \"bbox\": [\n",
        "#                     float(boxes[k][0]),\n",
        "#                     float(boxes[k][1]),\n",
        "#                     float(boxes[k][2] - boxes[k][0]),\n",
        "#                     float(boxes[k][3] - boxes[k][1])\n",
        "#                 ],\n",
        "#                 \"area\": float((boxes[k][2] - boxes[k][0]) * (boxes[k][3] - boxes[k][1])),\n",
        "#                 \"iscrowd\": 0\n",
        "#             }\n",
        "#             dataset[\"annotations\"].append(annotation)\n",
        "#             ann_id += 1\n",
        "\n",
        "#     coco_gt.dataset = dataset\n",
        "#     coco_gt.createIndex()\n",
        "\n",
        "#     coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
        "#     coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "#     coco_eval.evaluate()\n",
        "#     coco_eval.accumulate()\n",
        "#     coco_eval.summarize()\n",
        "\n",
        "#     class_aps = {}\n",
        "#     class_names = [\"aircraft\", \"oiltank\", \"overpass\", \"playground\"]\n",
        "\n",
        "#     for i, class_name in enumerate(class_names):\n",
        "#         coco_eval.params.catIds = [i]\n",
        "#         coco_eval.evaluate()\n",
        "#         coco_eval.accumulate()\n",
        "#         coco_eval.summarize()\n",
        "#         class_aps[class_name] = coco_eval.stats[1]  # AP@0.5 for this class\n",
        "\n",
        "#     return coco_eval.stats[0], class_aps  # mAP@0.5, per-class AP\n",
        "# # cell 10 : run final evaluation\n",
        "# mAP, class_aps = evaluate_coco(model, val_loader, device)\n",
        "\n",
        "# print(\"\\nFinal Evaluation Results:\")\n",
        "# print(f\"mAP@0.5: {mAP:.4f}\")\n",
        "# print(\"Per-class AP@0.5:\")\n",
        "# for class_name, ap in class_aps.items():\n",
        "#     print(f\"{class_name}: {ap:.4f}\")\n"
      ],
      "metadata": {
        "id": "TEX9bcX2K_Uo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📁 Set Dataset Paths"
      ],
      "metadata": {
        "id": "ioEmulXGLACL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 2 : set dataset paths\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "train_img_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/train/images\"\n",
        "train_ann_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/train/Annotations\"\n",
        "val_img_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/val/images\"\n",
        "val_ann_dir = \"/content/drive/MyDrive/csv/RSOD_YOLO/val/Annotations\"\n",
        "\n",
        "assert os.path.exists(train_img_dir)\n",
        "assert os.path.exists(train_ann_dir)\n",
        "assert os.path.exists(val_img_dir)\n",
        "assert os.path.exists(val_ann_dir)\n",
        "\n",
        "print(\"Dataset paths verified successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBVyZPcMLCDW",
        "outputId": "61e73f21-53a9-4f58-ab3d-46cc2f03ff29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset paths verified successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔄 Custom CLAHE Transform"
      ],
      "metadata": {
        "id": "puhQeKHSLD6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 3: custom clahe transform\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "\n",
        "class CLAHETransform(ImageOnlyTransform):\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8,8), always_apply=False, p=1.0):\n",
        "        super(CLAHETransform, self).__init__(always_apply=always_apply, p=p)\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "        cl = clahe.apply(l)\n",
        "        merged = cv2.merge((cl, a, b))\n",
        "        return cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    def get_transform_init_args_names(self):\n",
        "        return (\"clip_limit\", \"tile_grid_size\")\n"
      ],
      "metadata": {
        "id": "h0USKVncLE6X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔁 Transforms"
      ],
      "metadata": {
        "id": "HXBTHqZ_LGKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 4 : transforms\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        CLAHETransform(),\n",
        "        A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
        "        A.HueSaturationValue(20, 30, 20, p=0.5),\n",
        "        A.RGBShift(15, 15, 15, p=0.5),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Resize(640, 640),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "def get_val_transform():\n",
        "    return A.Compose([\n",
        "        CLAHETransform(),\n",
        "        A.Resize(640, 640),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n"
      ],
      "metadata": {
        "id": "vMElT29ULITS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📦 Dataset Class"
      ],
      "metadata": {
        "id": "YeilVYjzLKYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 5: dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class RSODDataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_dir, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annotation_dir = annotation_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = sorted(glob(os.path.join(image_dir, \"*.jpg\")) + glob(os.path.join(image_dir, \"*.png\")))\n",
        "        self.class_dict = {\"aircraft\": 1, \"oiltank\": 2, \"overpass\": 3, \"playground\": 4}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        xml_path = os.path.join(self.annotation_dir, os.path.basename(img_path).replace(\".jpg\", \".xml\").replace(\".png\", \".xml\"))\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        for obj in root.findall('object'):\n",
        "            class_name = obj.find('name').text.lower()\n",
        "            if class_name not in self.class_dict:\n",
        "                continue\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = float(bbox.find('xmin').text)\n",
        "            ymin = float(bbox.find('ymin').text)\n",
        "            xmax = float(bbox.find('xmax').text)\n",
        "            ymax = float(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(self.class_dict[class_name])\n",
        "\n",
        "        boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4))\n",
        "        labels = np.array(labels) if labels else np.zeros(0,)\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed[\"image\"]\n",
        "            boxes = torch.as_tensor(transformed[\"bboxes\"], dtype=torch.float32)\n",
        "            labels = torch.as_tensor(transformed[\"labels\"], dtype=torch.int64)\n",
        "\n",
        "        return image, {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "train_dataset = RSODDataset(train_img_dir, train_ann_dir, get_train_transform())\n",
        "val_dataset = RSODDataset(val_img_dir, val_ann_dir, get_val_transform())\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiuMgvhfLLiy",
        "outputId": "32f70ac5-776d-4e06-a996-a5a2ef8b162c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 749\n",
            "Validation samples: 187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-1982176329.py:9: UserWarning: Argument(s) 'always_apply' are not valid for transform BasicTransform\n",
            "  super(CLAHETransform, self).__init__(always_apply=always_apply, p=p)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Model Setup"
      ],
      "metadata": {
        "id": "4ew1KJSGLMMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 6 : meodel setup\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "def get_model(num_classes):\n",
        "    backbone = resnet_fpn_backbone('resnet101', pretrained=True, trainable_layers=5)\n",
        "    anchor_generator = AnchorGenerator(sizes=((32,), (64,), (128,), (256,), (512,)),\n",
        "                                       aspect_ratios=((0.5, 1.0, 2.0, 3.0),)*5)\n",
        "    roi_pooler = MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=7, sampling_ratio=2)\n",
        "    model = FasterRCNN(backbone, num_classes=num_classes,\n",
        "                       rpn_anchor_generator=anchor_generator,\n",
        "                       box_roi_pool=roi_pooler,\n",
        "                       box_score_thresh=0.05)\n",
        "    return model\n",
        "\n",
        "model = get_model(num_classes=5)\n"
      ],
      "metadata": {
        "id": "oVAAWFjoLOx0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔄 DataLoader and Optimizer"
      ],
      "metadata": {
        "id": "FSYGqvLuLQJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 7 : dataloader and optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "625-NmJOLSmE",
        "outputId": "243e5d7a-faa8-44b7-9011-35357cefe51a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (6): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (7): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (8): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (9): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (10): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (11): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (12): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (13): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (14): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (15): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (16): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (17): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (18): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (19): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (20): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (21): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (22): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🏋️ Training + Evaluation with Focal Loss"
      ],
      "metadata": {
        "id": "rOlLZdZkLTXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 8 : training + evaluation with focal loss\n",
        "import time\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class FocalLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    focal_loss = FocalLoss()\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = [img.to(device).float() for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Use model in training mode temporarily to compute loss\n",
        "            model.train()\n",
        "            loss_dict = model(images, targets)\n",
        "            model.eval()\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            eval_loss += losses.item()\n",
        "\n",
        "    return eval_loss / len(data_loader)\n",
        "\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Time: {time.time()-start:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJZO7OF2LVhR",
        "outputId": "d3b1e843-1da1-4813-e8cc-5cdf3da1d4c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Train: 0.8704 | Val: 0.7166 | Time: 309.95s\n",
            "Epoch 2/3 | Train: 0.6948 | Val: 0.6135 | Time: 252.15s\n",
            "Epoch 3/3 | Train: 0.5835 | Val: 0.5045 | Time: 251.71s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 COCO Evaluation"
      ],
      "metadata": {
        "id": "fqPeJLVoLc2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "\n",
        "def evaluate_coco(model, data_loader, device):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, targets) in enumerate(data_loader):\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for j, output in enumerate(outputs):\n",
        "                image_id = targets[j].get(\"image_id\", torch.tensor(i * len(images) + j)).item()\n",
        "                boxes = output[\"boxes\"].detach().cpu().numpy()\n",
        "                scores = output[\"scores\"].detach().cpu().numpy()\n",
        "                labels = output[\"labels\"].detach().cpu().numpy()\n",
        "\n",
        "                for k in range(len(boxes)):\n",
        "                    result = {\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(labels[k]),  # your labels are 1-indexed!\n",
        "                        \"bbox\": [\n",
        "                            float(boxes[k][0]),\n",
        "                            float(boxes[k][1]),\n",
        "                            float(boxes[k][2] - boxes[k][0]),\n",
        "                            float(boxes[k][3] - boxes[k][1])\n",
        "                        ],\n",
        "                        \"score\": float(scores[k])\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "    with open(\"predictions.json\", \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    val_image_ids = list(range(len(val_dataset)))\n",
        "    coco_gt = COCO()\n",
        "    dataset = {\n",
        "        \"info\": {\"description\": \"RSOD Validation\", \"version\": \"1.0\", \"year\": 2025},\n",
        "        \"licenses\": [],\n",
        "        \"images\": [{\"id\": i} for i in val_image_ids],\n",
        "        \"categories\": [\n",
        "            {\"id\": 1, \"name\": \"aircraft\"},\n",
        "            {\"id\": 2, \"name\": \"oiltank\"},\n",
        "            {\"id\": 3, \"name\": \"overpass\"},\n",
        "            {\"id\": 4, \"name\": \"playground\"}\n",
        "        ],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "    ann_id = 0\n",
        "    for img_id in val_image_ids:\n",
        "        _, target = val_dataset[img_id]\n",
        "        boxes = target[\"boxes\"].detach().numpy()\n",
        "        labels = target[\"labels\"].detach().numpy()\n",
        "\n",
        "        for k in range(len(boxes)):\n",
        "            annotation = {\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": int(labels[k]),  # make sure it's 1-indexed to match!\n",
        "                \"bbox\": [\n",
        "                    float(boxes[k][0]),\n",
        "                    float(boxes[k][1]),\n",
        "                    float(boxes[k][2] - boxes[k][0]),\n",
        "                    float(boxes[k][3] - boxes[k][1])\n",
        "                ],\n",
        "                \"area\": float((boxes[k][2] - boxes[k][0]) * (boxes[k][3] - boxes[k][1])),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            dataset[\"annotations\"].append(annotation)\n",
        "            ann_id += 1\n",
        "\n",
        "    coco_gt.dataset = dataset\n",
        "    coco_gt.createIndex()\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    class_aps = {}\n",
        "    class_names = [\"aircraft\", \"oiltank\", \"overpass\", \"playground\"]\n",
        "\n",
        "    for i, class_name in enumerate(class_names, start=1):  # start=1 matches your label IDs!\n",
        "        coco_eval.params.catIds = [i]\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        coco_eval.summarize()\n",
        "        class_aps[class_name] = coco_eval.stats[1]  # AP@0.5 for this class\n",
        "\n",
        "    return coco_eval.stats[0], class_aps  # mAP@0.5, per-class AP\n"
      ],
      "metadata": {
        "id": "GEM0ewxMknYL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Run Final Evaluation"
      ],
      "metadata": {
        "id": "9oA8wi2OLmAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 10 : run final evaluation\n",
        "mAP, class_aps = evaluate_coco(model, val_loader, device)\n",
        "\n",
        "print(\"\\nFinal Evaluation Results:\")\n",
        "print(f\"mAP@0.5: {mAP:.4f}\")\n",
        "print(\"Per-class AP@0.5:\")\n",
        "for class_name, ap in class_aps.items():\n",
        "    print(f\"{class_name}: {ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCr0VIAwLnED",
        "outputId": "10dd104d-03c9-467b-c2f5-77ccf269323d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.82s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.386\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.798\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.352\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.570\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.321\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.441\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.520\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.632\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.97s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.917\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.453\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.528\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.547\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.477\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.597\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.26s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.956\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.612\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.068\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.521\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.661\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.425\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.09s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.126\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.562\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.126\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.336\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.759\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.243\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.480\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.537\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.537\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537\n",
            "\n",
            "Final Evaluation Results:\n",
            "mAP@0.5: 0.3302\n",
            "Per-class AP@0.5:\n",
            "aircraft: 0.9169\n",
            "oiltank: 0.9556\n",
            "overpass: 0.5619\n",
            "playground: 0.7587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "images, targets = next(iter(val_loader))\n",
        "images = [img.to(device) for img in images]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "\n",
        "print(\"Predicted labels:\", [output[\"labels\"] for output in outputs])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjibnXo7mfhI",
        "outputId": "1fe4bf80-2b23-46b2-89d9-ca78f0f5795d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1,\n",
            "        3, 4, 1, 1], device='cuda:0'), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3,\n",
            "        3, 3, 1, 1, 1, 4, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1], device='cuda:0')]\n"
          ]
        }
      ]
    }
  ]
}